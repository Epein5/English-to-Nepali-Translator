{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import random\n",
    "import pandas as pd\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 8000  # Size of the English vocabulary\n",
    "OUTPUT_SIZE = 8000  # Size of the Nepali vocabulary\n",
    "EMBED_SIZE = 128\n",
    "HIDDEN_SIZE = 512\n",
    "N_LAYERS = 1\n",
    "DROPOUT = 0.5\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 50\n",
    "TEACHER_FORCING_RATIO = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, cleaned_file_path):\n",
    "        self.pairs = self.load_data(cleaned_file_path)\n",
    "\n",
    "    def load_data(self, cleaned_file_path):\n",
    "        df = pd.read_excel(cleaned_file_path)[:2]\n",
    "        df = df.dropna()\n",
    "        english_sentences = df['english_sent'].tolist()\n",
    "        nepali_sentences = df['nepali_sent'].tolist()\n",
    "\n",
    "        english_tokenizer = SentencePieceProcessor(model_file='english_sp.model')\n",
    "        nepali_tokenizer = SentencePieceProcessor(model_file='nepali_sp.model')\n",
    "\n",
    "        pairs = []\n",
    "        for english_sentence, nepali_sentence in zip(english_sentences, nepali_sentences):\n",
    "            english_indices = self.process_sentence(english_sentence, english_tokenizer)\n",
    "            nepali_indices = self.process_sentence(nepali_sentence, nepali_tokenizer)\n",
    "            pairs.append((english_indices, nepali_indices))\n",
    "        \n",
    "        return pairs\n",
    "\n",
    "    def process_sentence(self, sentence, tokenizer):\n",
    "        tokens = tokenizer.encode(sentence, out_type=int)\n",
    "        return [1] + tokens + [2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_lens = [len(src) for src in src_batch]\n",
    "    trg_lens = [len(trg) for trg in trg_batch]\n",
    "    src_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in src_batch], padding_value=0)\n",
    "    print(src_padded)\n",
    "    trg_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in trg_batch], padding_value=0)\n",
    "    return src_padded, trg_padded, src_lens, trg_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embed): Embedding(8000, 128)\n",
      "    (gru): GRU(128, 512, dropout=0.5, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embed): Embedding(8000, 128)\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    )\n",
      "    (gru): GRU(640, 512, dropout=0.5)\n",
      "    (out): Linear(in_features=1024, out_features=8000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/epein5/Data/English-to-Nepali-Translator/envenv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize model\n",
    "encoder = Encoder(INPUT_SIZE, EMBED_SIZE, HIDDEN_SIZE, N_LAYERS, DROPOUT).to(DEVICE)\n",
    "decoder = Decoder(EMBED_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, N_LAYERS, DROPOUT).to(DEVICE)\n",
    "model = Seq2Seq(encoder, decoder).to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop\n",
    "# def train_model(dataset):\n",
    "    \n",
    "#     dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "#     print(\"Data loaded...\")\n",
    "#     print(\"Training started...\")\n",
    "#     model.train()\n",
    "#     for epoch in range(EPOCHS):\n",
    "#         epoch_loss = 0\n",
    "#         for src, trg, _, _ in dataloader:\n",
    "#             src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(src, trg, TEACHER_FORCING_RATIO)\n",
    "#             output_dim = output.shape[-1]\n",
    "#             output = output[1:].view(-1, output_dim)\n",
    "#             trg = trg[1:].view(-1)\n",
    "#             loss = criterion(output, trg)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             # print(f\"Loss: {loss.item()}\")\n",
    "#             epoch_loss += loss.item()\n",
    "#         print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=\"runs/translation_experiment\")\n",
    "\n",
    "def train_model(dataset):\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "    print(\"Data loaded...\")\n",
    "    print(\"Training started...\")\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        for src, trg, _, _ in dataloader:\n",
    "            src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, trg, TEACHER_FORCING_RATIO)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Log loss to TensorBoard\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        writer.add_scalar(\"Loss/Train\", avg_loss, epoch)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "# torch.save(model.state_dict(), \"SavedModels/seq2seq_model2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, english_tokenizer, nepali_tokenizer, device ,BATCH_SIZE):\n",
    "    # Tokenize and encode the input sentence\n",
    "    tokens = english_tokenizer.encode(sentence, out_type=int)\n",
    "    english_indices = [1] + tokens + [2]  # Add <sos> and <eos> tokens\n",
    "    nepali_indices = [1]  # <sos> token\n",
    "    \n",
    "    pairs = []\n",
    "    pairs.append((english_indices, nepali_indices))\n",
    "    dataloader = DataLoader(pairs, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    a = 0\n",
    "    for src, trg, _, _ in dataloader:\n",
    "        a += 1\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        output = model(src, trg, 0)\n",
    "        print(output)\n",
    "        # print(output.shape)\n",
    "    print(a)\n",
    "    # return english_indices\n",
    "    # src_lens = [len(src) for src in english_indices]\n",
    "    # return src_lens\n",
    "\n",
    "    # Pass through the model\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     encoder_outputs, hidden = model.encoder(input_tensor)\n",
    "    #     decoder_input = torch.tensor([1]).to(device)  # <sos> token\n",
    "    #     decoder_hidden = hidden[:N_LAYERS]\n",
    "\n",
    "    #     translated_sentence = []\n",
    "    #     for _ in range(50):  # Max translation length\n",
    "    #         output, decoder_hidden, _ = model.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "    #         top1 = output.argmax(1)\n",
    "    #         translated_sentence.append(top1.item())\n",
    "    #         if top1.item() == 2:  # <eos> token\n",
    "    #             break\n",
    "    #         decoder_input = top1.unsqueeze(0)\n",
    "\n",
    "    # # Decode the token indices to words\n",
    "    # translated_words = nepali_tokenizer.decode(translated_sentence[:-1])  # Remove <eos> token\n",
    "    # return translated_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1],\n",
      "        [ 83],\n",
      "        [434],\n",
      "        [117],\n",
      "        [  4],\n",
      "        [420],\n",
      "        [ 33],\n",
      "        [ 17],\n",
      "        [ 84],\n",
      "        [  2]])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizers\n",
    "english_tokenizer = SentencePieceProcessor(model_file='english_sp.model')\n",
    "nepali_tokenizer = SentencePieceProcessor(model_file='nepali_sp.model')\n",
    "\n",
    "# Load the model\n",
    "model = Seq2Seq(encoder, decoder).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"SavedModels/model.pth\"))\n",
    "\n",
    "# Translate a sentence\n",
    "sentence = \"Hello, how are you?\"\n",
    "translation = translate_sentence(sentence, model, english_tokenizer, nepali_tokenizer, DEVICE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, english_tokenizer, nepali_tokenizer, device, max_length=50):\n",
    "    \"\"\"\n",
    "    Translate a single English sentence to Nepali\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Process input sentence\n",
    "    tokens = [1] + english_tokenizer.encode(sentence, out_type=int) + [2]\n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(1).to(device)\n",
    "    \n",
    "    # Get encoder outputs\n",
    "    encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "    hidden = hidden[:model.decoder.gru.num_layers]\n",
    "    \n",
    "    # Initialize decoder input\n",
    "    trg_indexes = [1]  # Start with SOS token\n",
    "    \n",
    "    # Initialize attention for visualization\n",
    "    attentions = torch.zeros(max_length, len(tokens))\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs)\n",
    "        \n",
    "        # Get predicted token\n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "        \n",
    "        # Save attention for visualization\n",
    "        attentions[i] = attention.squeeze()\n",
    "        \n",
    "        # Break if we predict EOS token\n",
    "        if pred_token == 2:\n",
    "            break\n",
    "    \n",
    "    # Convert tokens back to words\n",
    "    translated_tokens = trg_indexes[1:-1]  # Remove SOS and EOS tokens\n",
    "    translated_sentence = nepali_tokenizer.decode(translated_tokens)\n",
    "    \n",
    "    return translated_sentence, attentions[:len(trg_indexes)-1, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: David died in war\n",
      "Nepali: दाऊद दाऊद बेला म साह्रै युद्धमा। त्यस समय युद्धमा\n"
     ]
    }
   ],
   "source": [
    "# english_tokenizer, nepali_tokenizer = load_tokenizers()\n",
    "# model = load_model('best_model.pt', DEVICE)\n",
    "\n",
    "test_sentence = \"David died in war\"\n",
    "translated, attention = translate_sentence(\n",
    "    test_sentence, \n",
    "    model, \n",
    "    english_tokenizer, \n",
    "    nepali_tokenizer, \n",
    "    DEVICE\n",
    ")\n",
    "print(f\"English: {test_sentence}\")\n",
    "print(f\"Nepali: {translated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
