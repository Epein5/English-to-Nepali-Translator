{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import random\n",
    "import pandas as pd\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_SIZE = 10000  # Size of the English vocabulary\n",
    "# OUTPUT_SIZE = 10000  # Size of the Nepali vocabulary\n",
    "# EMBED_SIZE = 256\n",
    "# HIDDEN_SIZE = 512\n",
    "# N_LAYERS = 1\n",
    "# DROPOUT = 0.5\n",
    "# BATCH_SIZE = 4\n",
    "# LEARNING_RATE = 0.001\n",
    "# EPOCHS = 10\n",
    "# TEACHER_FORCING_RATIO = 0.5\n",
    "INPUT_SIZE = 8000  # Matches SentencePiece tokenizer vocabulary size\n",
    "OUTPUT_SIZE = 8000  # Matches SentencePiece tokenizer vocabulary size\n",
    "EMBED_SIZE = 128  # Reduce for memory constraints\n",
    "HIDDEN_SIZE = 256  # Reduce for memory constraints\n",
    "N_LAYERS = 1  # Single layer GRU\n",
    "DROPOUT = 0.3  # Prevent overfitting\n",
    "BATCH_SIZE = 2  # Reduce for memory constraints\n",
    "LEARNING_RATE = 0.001  # Standard learning rate\n",
    "EPOCHS = 10  # Initial testing\n",
    "TEACHER_FORCING_RATIO = 0.5  # Balanced teacher forcing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, cleaned_file_path):\n",
    "        self.pairs = self.load_data(cleaned_file_path)\n",
    "\n",
    "    def load_data(self, cleaned_file_path):\n",
    "        df = pd.read_excel(cleaned_file_path)\n",
    "        df = df.dropna()\n",
    "        english_sentences = df['english_sent'].tolist()\n",
    "        nepali_sentences = df['nepali_sent'].tolist()\n",
    "\n",
    "        english_tokenizer = SentencePieceProcessor(model_file='english_sp.model')\n",
    "        nepali_tokenizer = SentencePieceProcessor(model_file='nepali_sp.model')\n",
    "\n",
    "        pairs = []\n",
    "        for english_sentence, nepali_sentence in zip(english_sentences, nepali_sentences):\n",
    "            english_indices = self.process_sentence(english_sentence, english_tokenizer)\n",
    "            nepali_indices = self.process_sentence(nepali_sentence, nepali_tokenizer)\n",
    "            pairs.append((english_indices, nepali_indices))\n",
    "        \n",
    "        return pairs\n",
    "\n",
    "    def process_sentence(self, sentence, tokenizer):\n",
    "        tokens = tokenizer.encode(sentence, out_type=int)\n",
    "        return [1] + tokens + [2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_lens = [len(src) for src in src_batch]\n",
    "    trg_lens = [len(trg) for trg in trg_batch]\n",
    "    src_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in src_batch], padding_value=0)\n",
    "    trg_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in trg_batch], padding_value=0)\n",
    "    return src_padded, trg_padded, src_lens, trg_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/epein5/Data/English-to-Nepali-Translator/envenv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize model\n",
    "encoder = Encoder(INPUT_SIZE, EMBED_SIZE, HIDDEN_SIZE, N_LAYERS, DROPOUT).to(DEVICE)\n",
    "decoder = Decoder(EMBED_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, N_LAYERS, DROPOUT).to(DEVICE)\n",
    "model = Seq2Seq(encoder, decoder).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop\n",
    "# def train_model(dataset):\n",
    "    \n",
    "#     dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "#     print(\"Data loaded...\")\n",
    "#     print(\"Training started...\")\n",
    "#     model.train()\n",
    "#     for epoch in range(EPOCHS):\n",
    "#         epoch_loss = 0\n",
    "#         for src, trg, _, _ in dataloader:\n",
    "#             src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(src, trg, TEACHER_FORCING_RATIO)\n",
    "#             output_dim = output.shape[-1]\n",
    "#             output = output[1:].view(-1, output_dim)\n",
    "#             trg = trg[1:].view(-1)\n",
    "#             loss = criterion(output, trg)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             # print(f\"Loss: {loss.item()}\")\n",
    "#             epoch_loss += loss.item()\n",
    "#         print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=\"runs/translation_experiment\")\n",
    "\n",
    "def train_model(dataset):\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "    print(\"Data loaded...\")\n",
    "    print(\"Training started...\")\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        for src, trg, _, _ in dataloader:\n",
    "            src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, trg, TEACHER_FORCING_RATIO)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Log loss to TensorBoard\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        writer.add_scalar(\"Loss/Train\", avg_loss, epoch)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TranslationDataset(\"Dataset/english-nepali-cleaned.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded...\n",
      "Training started...\n",
      "Epoch 1, Loss: 5.7407\n",
      "Epoch 2, Loss: 5.1239\n",
      "Epoch 3, Loss: 4.9306\n",
      "Epoch 4, Loss: 4.8292\n",
      "Epoch 5, Loss: 4.7663\n",
      "Epoch 6, Loss: 4.7245\n",
      "Epoch 7, Loss: 4.6938\n",
      "Epoch 8, Loss: 4.6749\n",
      "Epoch 9, Loss: 4.6627\n",
      "Epoch 10, Loss: 5.1701\n"
     ]
    }
   ],
   "source": [
    "train_model(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"seq2seq_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, english_tokenizer, nepali_tokenizer, max_len=50):\n",
    "    \"\"\"\n",
    "    Translates an English sentence to Nepali.\n",
    "\n",
    "    Args:\n",
    "    - sentence (str): The input English sentence to translate.\n",
    "    - model (torch.nn.Module): The trained translation model.\n",
    "    - english_tokenizer (sentencepiece.SentencePieceProcessor): Tokenizer for English.\n",
    "    - nepali_tokenizer (sentencepiece.SentencePieceProcessor): Tokenizer for Nepali.\n",
    "    - max_len (int): Maximum length for the translated sentence.\n",
    "\n",
    "    Returns:\n",
    "    - str: The translated sentence in Nepali.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and encode the input sentence\n",
    "    src_tokens = [english_tokenizer.bos_id()] + english_tokenizer.encode(sentence) + [english_tokenizer.eos_id()]\n",
    "    src_tensor = torch.LongTensor(src_tokens).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # Prepare the initial target tensor\n",
    "    trg_tokens = [nepali_tokenizer.bos_id()]\n",
    "    trg_tensor = torch.LongTensor(trg_tokens).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            # Pass through the model\n",
    "            output = model(src_tensor, trg_tensor, 0)  # No teacher forcing during inference\n",
    "            next_token = output.argmax(2)[:, -1].item()\n",
    "\n",
    "            # Stop if EOS token is generated\n",
    "            if next_token == nepali_tokenizer.eos_id():\n",
    "                break\n",
    "\n",
    "            trg_tensor = torch.cat([trg_tensor, torch.LongTensor([[next_token]]).to(DEVICE)], dim=1)\n",
    "\n",
    "    # Decode the output tokens to Nepali sentence\n",
    "    translated_tokens = trg_tensor.squeeze(0).tolist()\n",
    "    print(translated_tokens)\n",
    "    translated_sentence = nepali_tokenizer.decode(translated_tokens[1:-1])  # Exclude BOS and EOS tokens\n",
    "\n",
    "    return translated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Translated Sentence:  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n"
     ]
    }
   ],
   "source": [
    "# Load your tokenizers\n",
    "import sentencepiece as spm\n",
    "english_tokenizer = spm.SentencePieceProcessor()\n",
    "english_tokenizer.load(\"english_sp.model\")\n",
    "\n",
    "nepali_tokenizer = spm.SentencePieceProcessor()\n",
    "nepali_tokenizer.load(\"nepali_sp.model\")\n",
    "\n",
    "# Example sentence\n",
    "english_sentence = \"How are you?\"\n",
    "\n",
    "# Translate\n",
    "nepali_translation = translate_sentence(\n",
    "    english_sentence, model, english_tokenizer, nepali_tokenizer\n",
    ")\n",
    "print(\"Translated Sentence:\", nepali_translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Tokens: [76, 12, 11, 1050, 5]\n",
      "Decoded Sentence: This is a test.\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"This is a test.\"\n",
    "tokens = english_tokenizer.encode(test_sentence)\n",
    "print(\"Encoded Tokens:\", tokens)\n",
    "decoded_sentence = english_tokenizer.decode(tokens)\n",
    "print(\"Decoded Sentence:\", decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
