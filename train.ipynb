{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import random\n",
    "import pandas as pd\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_SIZE = 10000  # Size of the English vocabulary\n",
    "# OUTPUT_SIZE = 10000  # Size of the Nepali vocabulary\n",
    "# EMBED_SIZE = 256\n",
    "# HIDDEN_SIZE = 512\n",
    "# N_LAYERS = 1\n",
    "# DROPOUT = 0.5\n",
    "# BATCH_SIZE = 4\n",
    "# LEARNING_RATE = 0.001\n",
    "# EPOCHS = 10\n",
    "# TEACHER_FORCING_RATIO = 0.5\n",
    "INPUT_SIZE = 8000  # Matches SentencePiece tokenizer vocabulary size\n",
    "OUTPUT_SIZE = 8000  # Matches SentencePiece tokenizer vocabulary size\n",
    "EMBED_SIZE = 128  # Reduce for memory constraints\n",
    "HIDDEN_SIZE = 256  # Reduce for memory constraints\n",
    "N_LAYERS = 1  # Single layer GRU\n",
    "DROPOUT = 0.3  # Prevent overfitting\n",
    "BATCH_SIZE = 2  # Reduce for memory constraints\n",
    "LEARNING_RATE = 0.001  # Standard learning rate\n",
    "EPOCHS = 10  # Initial testing\n",
    "TEACHER_FORCING_RATIO = 0.5  # Balanced teacher forcing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, cleaned_file_path):\n",
    "        self.pairs = self.load_data(cleaned_file_path)\n",
    "\n",
    "    def load_data(self, cleaned_file_path):\n",
    "        df = pd.read_excel(cleaned_file_path)\n",
    "        df = df.dropna()\n",
    "        english_sentences = df['english_sent'].tolist()\n",
    "        nepali_sentences = df['nepali_sent'].tolist()\n",
    "\n",
    "        english_tokenizer = SentencePieceProcessor(model_file='english_sp.model')\n",
    "        nepali_tokenizer = SentencePieceProcessor(model_file='nepali_sp.model')\n",
    "\n",
    "        pairs = []\n",
    "        for english_sentence, nepali_sentence in zip(english_sentences, nepali_sentences):\n",
    "            english_indices = self.process_sentence(english_sentence, english_tokenizer)\n",
    "            nepali_indices = self.process_sentence(nepali_sentence, nepali_tokenizer)\n",
    "            pairs.append((english_indices, nepali_indices))\n",
    "        \n",
    "        return pairs\n",
    "\n",
    "    def process_sentence(self, sentence, tokenizer):\n",
    "        tokens = tokenizer.encode(sentence, out_type=int)\n",
    "        return [1] + tokens + [2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_lens = [len(src) for src in src_batch]\n",
    "    trg_lens = [len(trg) for trg in trg_batch]\n",
    "    src_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in src_batch], padding_value=0)\n",
    "    trg_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in trg_batch], padding_value=0)\n",
    "    return src_padded, trg_padded, src_lens, trg_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/epein5/Data/English-to-Nepali-Translator/envenv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize model\n",
    "encoder = Encoder(INPUT_SIZE, EMBED_SIZE, HIDDEN_SIZE, N_LAYERS, DROPOUT).to(DEVICE)\n",
    "decoder = Decoder(EMBED_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, N_LAYERS, DROPOUT).to(DEVICE)\n",
    "model = Seq2Seq(encoder, decoder).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop\n",
    "# def train_model(dataset):\n",
    "    \n",
    "#     dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "#     print(\"Data loaded...\")\n",
    "#     print(\"Training started...\")\n",
    "#     model.train()\n",
    "#     for epoch in range(EPOCHS):\n",
    "#         epoch_loss = 0\n",
    "#         for src, trg, _, _ in dataloader:\n",
    "#             src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(src, trg, TEACHER_FORCING_RATIO)\n",
    "#             output_dim = output.shape[-1]\n",
    "#             output = output[1:].view(-1, output_dim)\n",
    "#             trg = trg[1:].view(-1)\n",
    "#             loss = criterion(output, trg)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             # print(f\"Loss: {loss.item()}\")\n",
    "#             epoch_loss += loss.item()\n",
    "#         print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=\"runs/translation_experiment\")\n",
    "\n",
    "def train_model(dataset):\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "    print(\"Data loaded...\")\n",
    "    print(\"Training started...\")\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        for src, trg, _, _ in dataloader:\n",
    "            src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, trg, TEACHER_FORCING_RATIO)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Log loss to TensorBoard\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        writer.add_scalar(\"Loss/Train\", avg_loss, epoch)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TranslationDataset(\"Dataset/english-nepali-cleaned.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded...\n",
      "Training started...\n"
     ]
    }
   ],
   "source": [
    "train_model(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
