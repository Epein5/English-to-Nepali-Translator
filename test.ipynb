{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Dataset/english-nepali.xlsx'\n",
    "cleaned_file_path = 'Dataset/english-nepali-cleaned.xlsx'\n",
    "df = pd.read_excel(cleaned_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sent</th>\n",
       "      <th>nepali_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It happened after the death of Saul, when Davi...</td>\n",
       "      <td>दाऊदले अमालेकीहरूलाई हराएर पछि सिकलग गए। यो शा...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it happened on the third day, that behold, a m...</td>\n",
       "      <td>तब तेस्रो दिनमा एउटा जवान सैनिक सिकलगमा आयो। त...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>David said to him, \"Where do you come from?\" H...</td>\n",
       "      <td>दाऊदले त्यसलाई सोधे, “तिमी कहाँबाट आयौ?” त्यस ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>David said to him, \"How did it go? Please tell...</td>\n",
       "      <td>दाऊदले भने, “मलाई भन, के भयो?” त्यसले भन्यो, “...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>David said to the young man who told him, \"How...</td>\n",
       "      <td>दाऊदले त्यस सैनिकलाई भने, “तिमीले कसरी जान्यौ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        english_sent  \\\n",
       "0  It happened after the death of Saul, when Davi...   \n",
       "1  it happened on the third day, that behold, a m...   \n",
       "2  David said to him, \"Where do you come from?\" H...   \n",
       "3  David said to him, \"How did it go? Please tell...   \n",
       "4  David said to the young man who told him, \"How...   \n",
       "\n",
       "                                         nepali_sent  \n",
       "0  दाऊदले अमालेकीहरूलाई हराएर पछि सिकलग गए। यो शा...  \n",
       "1  तब तेस्रो दिनमा एउटा जवान सैनिक सिकलगमा आयो। त...  \n",
       "2  दाऊदले त्यसलाई सोधे, “तिमी कहाँबाट आयौ?” त्यस ...  \n",
       "3  दाऊदले भने, “मलाई भन, के भयो?” त्यसले भन्यो, “...  \n",
       "4  दाऊदले त्यस सैनिकलाई भने, “तिमीले कसरी जान्यौ ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell at row 37024, column 'english_sent' is of type float with value: nan\n",
      "Cell at row 39886, column 'english_sent' is of type float with value: nan\n",
      "Cell at row 51028, column 'english_sent' is of type float with value: nan\n",
      "Cell at row 60299, column 'english_sent' is of type float with value: nan\n",
      "Cell at row 65642, column 'english_sent' is of type float with value: nan\n",
      "Cell at row 69008, column 'english_sent' is of type float with value: nan\n",
      "Cell at row 87382, column 'english_sent' is of type float with value: nan\n",
      "Cell at row 89873, column 'english_sent' is of type float with value: nan\n",
      "Cell at row 37024, column 'nepali_sent' is of type float with value: nan\n",
      "Cell at row 39886, column 'nepali_sent' is of type float with value: nan\n",
      "Cell at row 51028, column 'nepali_sent' is of type float with value: nan\n",
      "Cell at row 60299, column 'nepali_sent' is of type float with value: nan\n",
      "Cell at row 65642, column 'nepali_sent' is of type float with value: nan\n",
      "Cell at row 69008, column 'nepali_sent' is of type float with value: nan\n",
      "Cell at row 87382, column 'nepali_sent' is of type float with value: nan\n",
      "Cell at row 89873, column 'nepali_sent' is of type float with value: nan\n"
     ]
    }
   ],
   "source": [
    "# Check if all cells in the dataframe contain strings and print the cell that is not a string\n",
    "def check_cell_types(df):\n",
    "    for column in df.columns:\n",
    "        for index, value in df[column].items():\n",
    "            if not isinstance(value, str):\n",
    "                print(f\"Cell at row {index}, column '{column}' is of type {type(value).__name__} with value: {value}\")\n",
    "\n",
    "check_cell_types(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all cells in the dataframe contain strings and print the cell that is not a string\n",
    "def check_cell_types(df):\n",
    "    for column in df.columns:\n",
    "        for index, value in df[column].items():\n",
    "            if not isinstance(value, str):\n",
    "                print(f\"Cell at row {index}, column '{column}' is of type {type(value).__name__} with value: {value}\")\n",
    "\n",
    "check_cell_types(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_excel('Dataset/english-nepali-cleaned.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    english_sentences = df['english_sent'].tolist()\n",
    "    nepali_sentences = df['nepali_sent'].tolist()\n",
    "    \n",
    "    return english_sentences, nepali_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentences: ['It happened after the death of Saul, when David was returned from the slaughter of the Amalekites, and David had stayed two days in Ziklag;', 'it happened on the third day, that behold, a man came out of the camp from Saul, with his clothes torn, and earth on his head: and so it was, when he came to David, that he fell to the earth, and showed respect.', 'David said to him, \"Where do you come from?\" He said to him, \"I have escaped out of the camp of Israel.\"', 'David said to him, \"How did it go? Please tell me.\" He answered, \"The people have fled from the battle, and many of the people also have fallen and are dead; and Saul and Jonathan his son are dead also.\"', 'David said to the young man who told him, \"How do you know that Saul and Jonathan his son are dead?\"']\n",
      "Nepali Sentences: ['दाऊदले अमालेकीहरूलाई हराएर पछि सिकलग गए। यो शाऊलको मृत्यु भएको केही दिन पछिको कुरा हो। दाऊद त्यहाँ दुइ दिन बसे।', 'तब तेस्रो दिनमा एउटा जवान सैनिक सिकलगमा आयो। त्यो मानिस शाऊलको छाउनीबाट आएको थियो। त्यसका लुगाहरू च्यतिएको र शिरमा मैला लागेको थियो। त्यसले दाऊदको अघि धोप्टो परेर उनलाई सम्मान गर्न दण्डवत् गर्यो।', 'दाऊदले त्यसलाई सोधे, “तिमी कहाँबाट आयौ?” त्यस मानिसले जवाफ दियो, “म इस्राएली पालबाट आउँदैछु।”', 'दाऊदले भने, “मलाई भन, के भयो?” त्यसले भन्यो, “हाम्रा सबै सैनिकहरू भागे। धेरै मानिसहरू मारिए। शाऊल र तिनका छोरा जोनाथन पनि मरे।”', 'दाऊदले त्यस सैनिकलाई भने, “तिमीले कसरी जान्यौ शाऊल र जोनाथन मरेको कुरा?”']\n"
     ]
    }
   ],
   "source": [
    "english_sentences, nepali_sentences = load_data(file_path)\n",
    "\n",
    "print(\"English Sentences:\", english_sentences[:5])\n",
    "print(\"Nepali Sentences:\", nepali_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want nepali sentences to be saveed in a txt.txt file\n",
    "def save_data(file_path, data):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for sentence in data:\n",
    "            file.write(sentence + '\\n')\n",
    "\n",
    "# english_sentences, nepali_sentences = load_data(cleaned_file_path)\n",
    "save_data('Dataset/nepali_sentences.txt', nepali_sentences)\n",
    "save_data('Dataset/english_sentences.txt', english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8848, 5797, 17178, 21426, 28966, 5712, 9521, 23294, 9064, 12942, 9791, 11320, 1496, 8452, 8861, 5797, 9492, 9636, 8428, 8771, 8638, 22836, 8737, 8406, 2]\n"
     ]
    }
   ],
   "source": [
    "from nepalitokenizers import WordPiece\n",
    "\n",
    "text = \"दाऊदले अमालेकीहरूलाई हराएर पछि सिकलग गए। यो शाऊलको मृत्यु भएको केही दिन पछिको कुरा हो\"\n",
    "\n",
    "tokenizer_wp = WordPiece()\n",
    "\n",
    "tokens = tokenizer_wp.encode(text)\n",
    "# print(tokens.tokens)\n",
    "print(tokens.ids)\n",
    "print(tokens.tokens)\n",
    "\n",
    "# print(tokenizer_wp.decode(tokens.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'he', '##y', 'i', 'th', '##ink', 'i', 'am', 'in', 'love', 'with', 'you', '[SEP]']\n",
      "[1, 16066, 5816, 51, 9271, 9305, 51, 16151, 9470, 25483, 17704, 13631, 2]\n",
      "['[CLS]', 'he', '##y', 'i', 'th', '##ink', 'i', 'am', 'in', 'love', 'with', 'you', '[SEP]']\n",
      "hey i think i am in love with you\n"
     ]
    }
   ],
   "source": [
    "text = \"Hey i think i am in love with you\"\n",
    "tokens = tokenizer_wp.encode(text)\n",
    "print(tokens.tokens)\n",
    "print(tokens.ids)\n",
    "print(tokens.tokens)\n",
    "\n",
    "print(tokenizer_wp.decode(tokens.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentences: ['It happened after the death of Saul, when David was returned from the slaughter of the Amalekites, and David had stayed two days in Ziklag;', 'it happened on the third day, that behold, a man came out of the camp from Saul, with his clothes torn, and earth on his head: and so it was, when he came to David, that he fell to the earth, and showed respect.', 'David said to him, \"Where do you come from?\" He said to him, \"I have escaped out of the camp of Israel.\"', 'David said to him, \"How did it go? Please tell me.\" He answered, \"The people have fled from the battle, and many of the people also have fallen and are dead; and Saul and Jonathan his son are dead also.\"', 'David said to the young man who told him, \"How do you know that Saul and Jonathan his son are dead?\"']\n",
      "Nepali Sentences: ['दाऊदले अमालेकीहरूलाई हराएर पछि सिकलग गए। यो शाऊलको मृत्यु भएको केही दिन पछिको कुरा हो। दाऊद त्यहाँ दुइ दिन बसे।', 'तब तेस्रो दिनमा एउटा जवान सैनिक सिकलगमा आयो। त्यो मानिस शाऊलको छाउनीबाट आएको थियो। त्यसका लुगाहरू च्यतिएको र शिरमा मैला लागेको थियो। त्यसले दाऊदको अघि धोप्टो परेर उनलाई सम्मान गर्न दण्डवत् गर्यो।', 'दाऊदले त्यसलाई सोधे, “तिमी कहाँबाट आयौ?” त्यस मानिसले जवाफ दियो, “म इस्राएली पालबाट आउँदैछु।”', 'दाऊदले भने, “मलाई भन, के भयो?” त्यसले भन्यो, “हाम्रा सबै सैनिकहरू भागे। धेरै मानिसहरू मारिए। शाऊल र तिनका छोरा जोनाथन पनि मरे।”', 'दाऊदले त्यस सैनिकलाई भने, “तिमीले कसरी जान्यौ शाऊल र जोनाथन मरेको कुरा?”']\n"
     ]
    }
   ],
   "source": [
    "english_sentences, nepali_sentences = load_data(file_path)\n",
    "\n",
    "print(\"English Sentences:\", english_sentences[:5])\n",
    "print(\"Nepali Sentences:\", nepali_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: Dataset/english_sentences.txt\n",
      "  input_format: \n",
      "  model_prefix: english_sp\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: Dataset/english_sentences.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 151915 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=13904662\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9536% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=86\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999536\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 151915 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=7510807\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 158446 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 151915\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 118913\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 118913 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=61386 obj=10.924 num_tokens=252946 num_tokens/piece=4.12058\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=51936 obj=8.64012 num_tokens=253922 num_tokens/piece=4.88913\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=38945 obj=8.61783 num_tokens=266710 num_tokens/piece=6.84838\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=38923 obj=8.6043 num_tokens=266703 num_tokens/piece=6.85207\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=29192 obj=8.67345 num_tokens=286940 num_tokens/piece=9.82941\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=29191 obj=8.6582 num_tokens=286939 num_tokens/piece=9.82971\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=21893 obj=8.76122 num_tokens=310617 num_tokens/piece=14.188\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=21893 obj=8.73988 num_tokens=310592 num_tokens/piece=14.1868\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=16419 obj=8.88708 num_tokens=336247 num_tokens/piece=20.4791\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=16419 obj=8.85854 num_tokens=336257 num_tokens/piece=20.4797\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12314 obj=9.04795 num_tokens=362884 num_tokens/piece=29.4692\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12314 obj=9.01291 num_tokens=362901 num_tokens/piece=29.4706\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9235 obj=9.25343 num_tokens=390230 num_tokens/piece=42.2555\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9235 obj=9.20911 num_tokens=390254 num_tokens/piece=42.2581\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8800 obj=9.24917 num_tokens=394900 num_tokens/piece=44.875\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8800 obj=9.2421 num_tokens=394911 num_tokens/piece=44.8762\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: english_sp.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: english_sp.vocab\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: Dataset/nepali_sentences.txt\n",
      "  input_format: \n",
      "  model_prefix: nepali_sp\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: Dataset/nepali_sentences.txt\n",
      "trainer_interface.cc(380) LOG(WARNING) Found too long line (4353 > 4192).\n",
      "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 151910 sentences\n",
      "trainer_interface.cc(416) LOG(INFO) Skipped 5 too long sentences.\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=14131579\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9517% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=145\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999517\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 151910 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=7410142\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 313538 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 151910\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 200412\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 200412 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=96686 obj=12.7795 num_tokens=411391 num_tokens/piece=4.25492\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=80021 obj=10.4937 num_tokens=412684 num_tokens/piece=5.1572\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=59992 obj=10.4548 num_tokens=429475 num_tokens/piece=7.15887\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=59895 obj=10.4313 num_tokens=429668 num_tokens/piece=7.17369\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=44918 obj=10.5158 num_tokens=454750 num_tokens/piece=10.124\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=44916 obj=10.4941 num_tokens=454909 num_tokens/piece=10.128\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=33687 obj=10.6164 num_tokens=483357 num_tokens/piece=14.3485\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=33687 obj=10.5892 num_tokens=483348 num_tokens/piece=14.3482\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=25265 obj=10.7512 num_tokens=513913 num_tokens/piece=20.3409\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=25265 obj=10.7185 num_tokens=513918 num_tokens/piece=20.3411\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=18948 obj=10.9161 num_tokens=545726 num_tokens/piece=28.8012\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=18948 obj=10.8764 num_tokens=545745 num_tokens/piece=28.8022\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=14211 obj=11.1165 num_tokens=579146 num_tokens/piece=40.7534\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=14211 obj=11.0695 num_tokens=579156 num_tokens/piece=40.7541\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10658 obj=11.3573 num_tokens=614607 num_tokens/piece=57.6663\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10658 obj=11.3009 num_tokens=614630 num_tokens/piece=57.6684\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8800 obj=11.5119 num_tokens=638826 num_tokens/piece=72.5939\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8800 obj=11.4701 num_tokens=638829 num_tokens/piece=72.5942\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: nepali_sp.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: nepali_sp.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# For English\n",
    "spm.SentencePieceTrainer.train(input='Dataset/english_sentences.txt', model_prefix='english_sp', vocab_size=8000)\n",
    "\n",
    "# For Nepali\n",
    "spm.SentencePieceTrainer.train(input='Dataset/nepali_sentences.txt', model_prefix='nepali_sp', vocab_size=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English indices: [1, 334, 61, 8, 52, 4, 66, 4566, 95, 17, 151, 36, 3044, 2]\n",
      "Nepali indices: [1, 1114, 774, 1990, 7, 137, 2977, 819, 21, 1025, 166, 107, 244, 2]\n"
     ]
    }
   ],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "# Load tokenizers for English and Nepali\n",
    "english_tokenizer = SentencePieceProcessor(model_file='english_sp.model')\n",
    "nepali_tokenizer = SentencePieceProcessor(model_file='nepali_sp.model')\n",
    "\n",
    "# Tokenize and convert to indices\n",
    "def process_sentence(sentence, tokenizer):\n",
    "    tokens = tokenizer.encode(sentence, out_type=int)\n",
    "    return [1] + tokens + [2]  # Add <SOS> (1) and <EOS> (2)\n",
    "\n",
    "english_sentence = \"David said to him, 'Where do you come from?'\"\n",
    "nepali_sentence = \"दाऊदले त्यसलाई सोधे, 'तिमी कहाँबाट आयौ?'\"\n",
    "\n",
    "english_indices = process_sentence(english_sentence, english_tokenizer)\n",
    "nepali_indices = process_sentence(nepali_sentence, nepali_tokenizer)\n",
    "\n",
    "print(\"English indices:\", english_indices)\n",
    "print(\"Nepali indices:\", nepali_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def process_sentence(sentence, tokenizer):\n",
    "        tokens = tokenizer.encode(sentence, out_type=int)\n",
    "        return [1] + tokens + [2]\n",
    "    \n",
    "    df = pd.read_excel(cleaned_file_path)\n",
    "    df = df.dropna()\n",
    "    english_sentences = df['english_sent'].tolist()\n",
    "    nepali_sentences = df['nepali_sent'].tolist()\n",
    "\n",
    "    english_tokenizer = SentencePieceProcessor(model_file='english_sp.model')\n",
    "    nepali_tokenizer = SentencePieceProcessor(model_file='nepali_sp.model')\n",
    "\n",
    "    pairs = []\n",
    "    for english_sentence, nepali_sentence in zip(english_sentences, nepali_sentences):\n",
    "        english_indices = process_sentence(english_sentence, english_tokenizer)\n",
    "        nepali_indices = process_sentence(nepali_sentence, nepali_tokenizer)\n",
    "        pairs.append((english_indices, nepali_indices))\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentenc = df['english_sent'].tolist()\n",
    "nepali_sentenc = df['nepali_sent'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151915, 151915)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_sentenc), len(nepali_sentenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, cleaned_file_path):\n",
    "        self.pairs = self.load_data(cleaned_file_path)\n",
    "\n",
    "    def load_data(self, cleaned_file_path):\n",
    "        df = pd.read_excel(cleaned_file_path)\n",
    "        df = df.dropna()\n",
    "        english_sentences = df['english_sent'].tolist()\n",
    "        nepali_sentences = df['nepali_sent'].tolist()\n",
    "\n",
    "        english_tokenizer = SentencePieceProcessor(model_file='english_sp.model')\n",
    "        nepali_tokenizer = SentencePieceProcessor(model_file='nepali_sp.model')\n",
    "\n",
    "        pairs = []\n",
    "        for english_sentence, nepali_sentence in zip(english_sentences, nepali_sentences):\n",
    "            english_indices = self.process_sentence(english_sentence, english_tokenizer)\n",
    "            nepali_indices = self.process_sentence(nepali_sentence, nepali_tokenizer)\n",
    "            pairs.append((english_indices, nepali_indices))\n",
    "        \n",
    "        return pairs\n",
    "\n",
    "    def process_sentence(self, sentence, tokenizer):\n",
    "        tokens = tokenizer.encode(sentence, out_type=int)\n",
    "        return [1] + tokens + [2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = TranslationDataset(cleaned_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([1, 93, 715, 129, 3, 695, 6, 1044, 4, 92, 334, 37, 1373, 36, 3, 4934, 6, 3, 6007, 880, 4, 9, 334, 97, 3453, 152, 279, 10, 3222, 123, 237, 111, 38, 2], [1, 1114, 7479, 12, 3700, 27, 274, 1601, 2178, 368, 4, 19, 995, 3, 667, 34, 79, 113, 274, 3, 103, 32, 4, 948, 171, 1247, 113, 1374, 4, 2]), ([1, 30, 715, 25, 3, 1032, 133, 4, 20, 760, 4, 11, 115, 165, 79, 6, 3, 1679, 36, 1044, 4, 26, 44, 2439, 5316, 4, 9, 380, 25, 44, 512, 27, 9, 137, 30, 37, 4, 92, 43, 165, 8, 334, 4, 20, 43, 1355, 8, 3, 380, 4, 9, 3143, 1815, 5, 2], [1, 141, 1535, 1925, 52, 2011, 2333, 1601, 2178, 6, 1242, 4, 59, 181, 995, 3, 2336, 21, 443, 43, 4, 75, 10, 4432, 11, 5584, 67, 295, 9, 2271, 6, 6104, 1378, 43, 4, 75, 8, 948, 3, 402, 2644, 3800, 65, 1555, 27, 996, 1277, 14, 742, 5855, 785, 4, 2]), ([1, 334, 61, 8, 52, 4, 41, 4566, 95, 17, 151, 36, 449, 83, 61, 8, 52, 4, 41, 178, 35, 5130, 79, 6, 3, 1679, 6, 128, 94, 2], [1, 1114, 774, 1990, 7, 37, 2977, 819, 21, 1025, 166, 734, 75, 511, 894, 1947, 7, 37, 38, 1750, 994, 21, 6238, 60, 73, 2]), ([1, 334, 61, 8, 52, 4, 41, 3931, 231, 30, 170, 84, 318, 515, 74, 94, 83, 840, 4, 41, 837, 80, 35, 2778, 36, 3, 2046, 4, 9, 218, 6, 3, 80, 67, 35, 2983, 9, 33, 804, 38, 9, 1044, 9, 3297, 44, 143, 33, 804, 67, 94, 2], [1, 1114, 31, 7, 37, 38, 12, 684, 7, 104, 57, 734, 75, 8, 1625, 7, 37, 7846, 45, 1996, 4282, 4, 101, 87, 3966, 4, 995, 9, 1003, 229, 3440, 17, 1524, 73, 2]), ([1, 334, 61, 8, 3, 668, 115, 47, 861, 52, 4, 41, 3931, 95, 17, 211, 20, 1044, 9, 3297, 44, 143, 33, 804, 449, 2], [1, 1114, 75, 2333, 12, 31, 7, 37, 3396, 756, 464, 1262, 995, 9, 3440, 1524, 3, 103, 734, 2])]\n"
     ]
    }
   ],
   "source": [
    "print(a[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "def load_data(cleaned_file_path):\n",
    "        df = pd.read_excel(cleaned_file_path)\n",
    "        df = df.dropna()\n",
    "        english_sentences = df['english_sent'].tolist()\n",
    "        nepali_sentences = df['nepali_sent'].tolist()\n",
    "\n",
    "        english_tokenizer = SentencePieceProcessor(model_file='english_sp.model')\n",
    "        nepali_tokenizer = SentencePieceProcessor(model_file='nepali_sp.model')\n",
    "        def process_sentence(sentence, tokenizer):\n",
    "            tokens = tokenizer.encode(sentence, out_type=int)\n",
    "            return [1] + tokens + [2]\n",
    "\n",
    "        pairs = []\n",
    "        largest_english_indices = 0\n",
    "        largest_nepali_indices = 0\n",
    "        for english_sentence, nepali_sentence in zip(english_sentences, nepali_sentences):\n",
    "            english_indices = process_sentence(english_sentence, english_tokenizer)\n",
    "            nepali_indices = process_sentence(nepali_sentence, nepali_tokenizer)\n",
    "            # pairs.append((english_indices, nepali_indices))\n",
    "            pairs.append((len(english_indices), len(nepali_indices)))\n",
    "            if len(english_indices) > largest_english_indices:\n",
    "                largest_english_indices = len(english_indices)\n",
    "            if len(nepali_indices) > largest_nepali_indices:\n",
    "                largest_nepali_indices = len(nepali_indices)\n",
    "        # print(a, b)\n",
    "        \n",
    "        return largest_english_indices, largest_nepali_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(622, 730)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cleaned_file_path = 'Dataset/english-nepali-cleaned.xlsx'\n",
    "\n",
    "print(load_data(cleaned_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
